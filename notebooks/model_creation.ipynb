{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pawel\\anaconda3\\envs\\env_torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/train_sample_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "statuses = np.unique(df['OpenStatus'].values)\n",
    "\n",
    "id2label = {idx: label for idx, label in enumerate(statuses)}\n",
    "label2id = {label: idx for idx, label in enumerate(statuses)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from custom_dataset import GithubDataset\n",
    "from torch.utils.data import random_split\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create the pytorch dataset\n",
    "full_dataset = GithubDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset, test_dataset = random_split(full_dataset, [0.7, 0.25, 0.05])\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_dataset[:])\n",
    "validation_dataset = Dataset.from_dict(validation_dataset[:])\n",
    "test_dataset = Dataset.from_dict(test_dataset[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.rename_columns({\"text_content\": \"text\", \"status\": \"label\"})\n",
    "validation_dataset = validation_dataset.rename_columns({\"text_content\": \"text\", \"status\": \"label\"})\n",
    "test_dataset = test_dataset.rename_columns({\"text_content\": \"text\", \"status\": \"label\"})\n",
    "\n",
    "columns_to_remove = [\n",
    "  'tags_onehot',\n",
    "  'unrecognized_tags_count',\n",
    "  'reputation',\n",
    "  'undeleted_answers',\n",
    "  'user_life_days',\n",
    "  'title'\n",
    "]\n",
    "\n",
    "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
    "validation_dataset = validation_dataset.remove_columns(columns_to_remove)\n",
    "test_dataset = test_dataset.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TEXT_CONTENT = 128\n",
    "\n",
    "def tokenize_func(batch):\n",
    "  tokenized_batch = tokenizer(batch['text'], padding=True, truncation=True, max_length=MAX_TEXT_CONTENT, return_tensors=\"pt\")\n",
    "  tokenized_batch[\"labels\"] = [label2id[label] for label in batch[\"label\"]]\n",
    "  return tokenized_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 98191/98191 [00:14<00:00, 6648.58 examples/s]\n",
      "Map: 100%|██████████| 35068/35068 [00:05<00:00, 6522.01 examples/s]\n",
      "Map: 100%|██████████| 7013/7013 [00:01<00:00, 6950.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(tokenize_func, batched=True)\n",
    "tokenized_validation_dataset = validation_dataset.map(tokenize_func, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMMENT - IDEAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably should:\n",
    "- retrain the whole model (probably smaller) with\n",
    "- better tokenizer - built up from the ground including all the names of the specific tech (languages, frameworks, IDEs, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_model import AutoCompositeModel\n",
    "\n",
    "model = AutoCompositeModel(device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor(tokenized_test_dataset['input_ids'][:10]).float().to(device)\n",
    "input.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "training_loader = DataLoader(tokenized_train_dataset, batch_size=16, shuffle=True)\n",
    "validation_loader = DataLoader(tokenized_validation_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(tokenized_test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-5, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "accuracy_metric = Accuracy(task='multiclass', num_classes=5).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_own import Trainer, TrainerConfiguration, get_model_params\n",
    "\n",
    "config = TrainerConfiguration(\n",
    "  training_loader=training_loader,\n",
    "  validation_loader=validation_loader,\n",
    "  optimizer=optimizer,\n",
    "  loss_fn=loss_fn,\n",
    "  accuracy_metric=accuracy_metric,\n",
    "  device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1777989"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, trainer_configuration=config, input_column='input_ids', output_column='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6137"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 300 training_loss: 1.5919880652427674 training_accuracy: 0.38104167580604553\n",
      " batch 600 training_loss: 1.5863707367579143 training_accuracy: 0.49166667461395264\n",
      " batch 900 training_loss: 1.5808382944266002 training_accuracy: 0.49041667580604553\n",
      " batch 1200 training_loss: 1.5758536382516226 training_accuracy: 0.4985416829586029\n",
      " batch 1500 training_loss: 1.5684045406182607 training_accuracy: 0.5062500238418579\n",
      " batch 1800 training_loss: 1.56304585536321 training_accuracy: 0.5014583468437195\n",
      " batch 2100 training_loss: 1.5569155955314635 training_accuracy: 0.5110417008399963\n",
      " batch 2400 training_loss: 1.5525289980570476 training_accuracy: 0.5054166913032532\n",
      " batch 2700 training_loss: 1.5490184779961904 training_accuracy: 0.4931250214576721\n",
      " batch 3000 training_loss: 1.5403145523866018 training_accuracy: 0.5116666555404663\n",
      " batch 3300 training_loss: 1.5388207133611043 training_accuracy: 0.49979168176651\n",
      " batch 3600 training_loss: 1.5349925100803374 training_accuracy: 0.4910416901111603\n",
      " batch 3900 training_loss: 1.5260915414492289 training_accuracy: 0.5079166889190674\n",
      " batch 4200 training_loss: 1.5239989840984345 training_accuracy: 0.50020831823349\n",
      " batch 4500 training_loss: 1.5225679755210877 training_accuracy: 0.48770835995674133\n",
      " batch 4800 training_loss: 1.5139369750022889 training_accuracy: 0.5037500262260437\n",
      " batch 5100 training_loss: 1.5091695964336396 training_accuracy: 0.5043750405311584\n",
      " batch 5400 training_loss: 1.507971570889155 training_accuracy: 0.4925000071525574\n",
      " batch 5700 training_loss: 1.4977274564901988 training_accuracy: 0.5104166865348816\n",
      " batch 6000 training_loss: 1.4929158953825632 training_accuracy: 0.5127083659172058\n",
      "LOSS train 1.4929158953825632 valid 1.4942233562469482 ACCURACY train 0.4949568212032318 validation 0.49773797392845154\n",
      "EPOCH 2:\n",
      " batch 300 training_loss: 1.4925246647993724 training_accuracy: 0.49166667461395264\n",
      " batch 600 training_loss: 1.4850725217660268 training_accuracy: 0.5099999904632568\n",
      " batch 900 training_loss: 1.4792247410615285 training_accuracy: 0.5097916722297668\n",
      " batch 1200 training_loss: 1.4754160058498382 training_accuracy: 0.5083333253860474\n",
      " batch 1500 training_loss: 1.4744757239023845 training_accuracy: 0.5012500286102295\n",
      " batch 1800 training_loss: 1.4727327672640482 training_accuracy: 0.4970833361148834\n",
      " batch 2100 training_loss: 1.4710140820344288 training_accuracy: 0.4935416877269745\n",
      " batch 2400 training_loss: 1.4645632644494375 training_accuracy: 0.5008333325386047\n",
      " batch 2700 training_loss: 1.4609826719760894 training_accuracy: 0.49770835041999817\n",
      " batch 3000 training_loss: 1.4587126469612122 training_accuracy: 0.49645835161209106\n",
      " batch 3300 training_loss: 1.4611970925331115 training_accuracy: 0.4816666841506958\n",
      " batch 3600 training_loss: 1.452098507086436 training_accuracy: 0.50020831823349\n",
      " batch 3900 training_loss: 1.4413255389531454 training_accuracy: 0.5104166865348816\n",
      " batch 4200 training_loss: 1.4418239164352418 training_accuracy: 0.5039583444595337\n",
      " batch 4500 training_loss: 1.4396898142496746 training_accuracy: 0.49958333373069763\n",
      " batch 4800 training_loss: 1.4366405391693116 training_accuracy: 0.49937501549720764\n",
      " batch 5100 training_loss: 1.4308741879463196 training_accuracy: 0.5037500262260437\n",
      " batch 5400 training_loss: 1.4247295753161113 training_accuracy: 0.5062500238418579\n",
      " batch 5700 training_loss: 1.425389142036438 training_accuracy: 0.5014583468437195\n",
      " batch 6000 training_loss: 1.4196150195598602 training_accuracy: 0.5091666579246521\n",
      "LOSS train 1.4196150195598602 valid 1.4218308925628662 ACCURACY train 0.5011460781097412 validation 0.4977664649486542\n",
      "EPOCH 3:\n",
      " batch 300 training_loss: 1.4230924872557322 training_accuracy: 0.4922916889190674\n",
      " batch 600 training_loss: 1.4071400487422943 training_accuracy: 0.512291669845581\n",
      " batch 900 training_loss: 1.412832740942637 training_accuracy: 0.503333330154419\n",
      " batch 1200 training_loss: 1.4060888516902923 training_accuracy: 0.503125011920929\n",
      " batch 1500 training_loss: 1.3991390117009481 training_accuracy: 0.512291669845581\n",
      " batch 1800 training_loss: 1.3967238541444142 training_accuracy: 0.5060417056083679\n",
      " batch 2100 training_loss: 1.403794188896815 training_accuracy: 0.4947916865348816\n",
      " batch 2400 training_loss: 1.3947790046532949 training_accuracy: 0.5043750405311584\n",
      " batch 2700 training_loss: 1.398385149637858 training_accuracy: 0.4960416853427887\n",
      " batch 3000 training_loss: 1.3955632666746776 training_accuracy: 0.491875022649765\n",
      " batch 3300 training_loss: 1.3855830148855846 training_accuracy: 0.5037500262260437\n",
      " batch 3600 training_loss: 1.397733983596166 training_accuracy: 0.4866666793823242\n",
      " batch 3900 training_loss: 1.3799107813835143 training_accuracy: 0.5093750357627869\n",
      " batch 4200 training_loss: 1.3818915895620982 training_accuracy: 0.5010416507720947\n",
      " batch 4500 training_loss: 1.3782368890444439 training_accuracy: 0.5054166913032532\n",
      " batch 4800 training_loss: 1.383816347916921 training_accuracy: 0.49166667461395264\n",
      " batch 5100 training_loss: 1.3812141172091166 training_accuracy: 0.492083340883255\n",
      " batch 5400 training_loss: 1.375764673948288 training_accuracy: 0.49916666746139526\n",
      " batch 5700 training_loss: 1.3610905945301055 training_accuracy: 0.5099999904632568\n",
      " batch 6000 training_loss: 1.3565199518203734 training_accuracy: 0.5131250023841858\n",
      "LOSS train 1.3565199518203734 valid 1.3682963848114014 ACCURACY train 0.5011460781097412 validation 0.49773797392845154\n",
      "EPOCH 4:\n",
      " batch 300 training_loss: 1.3670240759849548 training_accuracy: 0.4985416829586029\n",
      " batch 600 training_loss: 1.3692572180430094 training_accuracy: 0.4958333373069763\n",
      " batch 900 training_loss: 1.354029988447825 training_accuracy: 0.5072916746139526\n",
      " batch 1200 training_loss: 1.35356299161911 training_accuracy: 0.5045833587646484\n",
      " batch 1500 training_loss: 1.3474340748786926 training_accuracy: 0.5064583420753479\n",
      " batch 1800 training_loss: 1.3518237495422363 training_accuracy: 0.5054166913032532\n",
      " batch 2100 training_loss: 1.3451490239302317 training_accuracy: 0.5120833516120911\n",
      " batch 2400 training_loss: 1.351609758536021 training_accuracy: 0.5\n",
      " batch 2700 training_loss: 1.3591648014386495 training_accuracy: 0.49270835518836975\n",
      " batch 3000 training_loss: 1.3420510884126027 training_accuracy: 0.5064583420753479\n",
      " batch 3300 training_loss: 1.3433144875367482 training_accuracy: 0.5010416507720947\n",
      " batch 3600 training_loss: 1.3520722258090974 training_accuracy: 0.49541667103767395\n",
      " batch 3900 training_loss: 1.3511241801579794 training_accuracy: 0.4943750202655792\n",
      " batch 4200 training_loss: 1.344983048439026 training_accuracy: 0.49791666865348816\n",
      " batch 4500 training_loss: 1.326391887863477 training_accuracy: 0.5147916674613953\n",
      " batch 4800 training_loss: 1.339374625881513 training_accuracy: 0.5047916769981384\n",
      " batch 5100 training_loss: 1.3463883872826894 training_accuracy: 0.49416667222976685\n",
      " batch 5400 training_loss: 1.3487678617238998 training_accuracy: 0.4891666769981384\n",
      " batch 5700 training_loss: 1.3284531849622727 training_accuracy: 0.5066666603088379\n",
      " batch 6000 training_loss: 1.3391995181639988 training_accuracy: 0.5020833611488342\n",
      "LOSS train 1.3391995181639988 valid 1.3402150869369507 ACCURACY train 0.5011453628540039 validation 0.4977855086326599\n",
      "EPOCH 5:\n",
      " batch 300 training_loss: 1.3317077928781509 training_accuracy: 0.5037500262260437\n",
      " batch 600 training_loss: 1.3293010658025741 training_accuracy: 0.5060417056083679\n",
      " batch 900 training_loss: 1.3384859919548036 training_accuracy: 0.49979168176651\n",
      " batch 1200 training_loss: 1.3433678177992503 training_accuracy: 0.4933333396911621\n",
      " batch 1500 training_loss: 1.3419059556722641 training_accuracy: 0.49687501788139343\n",
      " batch 1800 training_loss: 1.3341228691736857 training_accuracy: 0.4989583492279053\n",
      " batch 2100 training_loss: 1.3464535389343897 training_accuracy: 0.4925000071525574\n",
      " batch 2400 training_loss: 1.3267633958657583 training_accuracy: 0.5041666626930237\n",
      " batch 2700 training_loss: 1.3372030079364776 training_accuracy: 0.49979168176651\n",
      " batch 3000 training_loss: 1.3325992812712988 training_accuracy: 0.4987500011920929\n",
      " batch 3300 training_loss: 1.3217280608415605 training_accuracy: 0.5062500238418579\n",
      " batch 3600 training_loss: 1.3364942562580109 training_accuracy: 0.4958333373069763\n",
      " batch 3900 training_loss: 1.3274857958157857 training_accuracy: 0.502916693687439\n",
      " batch 4200 training_loss: 1.3333168278137844 training_accuracy: 0.49937501549720764\n",
      " batch 4500 training_loss: 1.2897077240546544 training_accuracy: 0.5295833349227905\n",
      " batch 4800 training_loss: 1.3350551096598307 training_accuracy: 0.5004166960716248\n",
      " batch 5100 training_loss: 1.3351451949278514 training_accuracy: 0.4958333373069763\n",
      " batch 5400 training_loss: 1.3187070433298747 training_accuracy: 0.5052083730697632\n",
      " batch 5700 training_loss: 1.3318298842509588 training_accuracy: 0.49791666865348816\n",
      " batch 6000 training_loss: 1.3312492277224859 training_accuracy: 0.4958333373069763\n",
      "LOSS train 1.3312492277224859 valid 1.3319988250732422 ACCURACY train 0.5011467337608337 validation 0.4977570176124573\n",
      "EPOCH 6:\n",
      " batch 300 training_loss: 1.3326613610982896 training_accuracy: 0.4947916865348816\n",
      " batch 600 training_loss: 1.3325428301095963 training_accuracy: 0.5008333325386047\n",
      " batch 900 training_loss: 1.3393420710166295 training_accuracy: 0.4906250238418579\n",
      " batch 1200 training_loss: 1.3402433824539184 training_accuracy: 0.4922916889190674\n",
      " batch 1500 training_loss: 1.3301954225699106 training_accuracy: 0.5010416507720947\n",
      " batch 1800 training_loss: 1.316840271949768 training_accuracy: 0.5095833539962769\n",
      " batch 2100 training_loss: 1.3215069935719173 training_accuracy: 0.5039583444595337\n",
      " batch 2400 training_loss: 1.311263004541397 training_accuracy: 0.5081250071525574\n",
      " batch 2700 training_loss: 1.3263664986689885 training_accuracy: 0.49979168176651\n",
      " batch 3000 training_loss: 1.3314477376143137 training_accuracy: 0.5008333325386047\n",
      " batch 3300 training_loss: 1.323046157360077 training_accuracy: 0.5062500238418579\n",
      " batch 3600 training_loss: 1.328070897857348 training_accuracy: 0.4987500011920929\n",
      " batch 3900 training_loss: 1.317383781870206 training_accuracy: 0.5064583420753479\n",
      " batch 4200 training_loss: 1.3233626258373261 training_accuracy: 0.5035417079925537\n",
      " batch 4500 training_loss: 1.327863336801529 training_accuracy: 0.5018749833106995\n",
      " batch 4800 training_loss: 1.3365234112739564 training_accuracy: 0.4879166781902313\n",
      " batch 5100 training_loss: 1.3286644214391707 training_accuracy: 0.4985416829586029\n",
      " batch 5400 training_loss: 1.3266284704208373 training_accuracy: 0.5024999976158142\n",
      " batch 5700 training_loss: 1.3088826783498129 training_accuracy: 0.5087500214576721\n",
      " batch 6000 training_loss: 1.3117314819494883 training_accuracy: 0.502916693687439\n",
      "LOSS train 1.3117314819494883 valid 1.3285210132598877 ACCURACY train 0.5011453628540039 validation 0.49777600169181824\n",
      "EPOCH 7:\n",
      " batch 300 training_loss: 1.3313308374087016 training_accuracy: 0.49916666746139526\n",
      " batch 600 training_loss: 1.3234138737122219 training_accuracy: 0.5047916769981384\n",
      " batch 900 training_loss: 1.323369347055753 training_accuracy: 0.5004166960716248\n",
      " batch 1200 training_loss: 1.3196393338839214 training_accuracy: 0.503125011920929\n",
      " batch 1500 training_loss: 1.325380253593127 training_accuracy: 0.4945833384990692\n",
      " batch 1800 training_loss: 1.3266223124663035 training_accuracy: 0.5020833611488342\n",
      " batch 2100 training_loss: 1.30999407072862 training_accuracy: 0.5104166865348816\n",
      " batch 2400 training_loss: 1.3088340697685878 training_accuracy: 0.5137500166893005\n",
      " batch 2700 training_loss: 1.3289203520615895 training_accuracy: 0.49916666746139526\n",
      " batch 3000 training_loss: 1.3174284420410791 training_accuracy: 0.5052083730697632\n",
      " batch 3300 training_loss: 1.3226514442761739 training_accuracy: 0.503125011920929\n",
      " batch 3600 training_loss: 1.326467099984487 training_accuracy: 0.49520835280418396\n",
      " batch 3900 training_loss: 1.3410012735923131 training_accuracy: 0.48625001311302185\n",
      " batch 4200 training_loss: 1.3214993290106456 training_accuracy: 0.49979168176651\n",
      " batch 4500 training_loss: 1.3298271723588309 training_accuracy: 0.49812501668930054\n",
      " batch 4800 training_loss: 1.3340561739603678 training_accuracy: 0.4866666793823242\n",
      " batch 5100 training_loss: 1.3246661911408106 training_accuracy: 0.49979168176651\n",
      " batch 5400 training_loss: 1.3193721882502238 training_accuracy: 0.5056250095367432\n",
      " batch 5700 training_loss: 1.3043527166048685 training_accuracy: 0.5129166841506958\n",
      " batch 6000 training_loss: 1.3203193817536036 training_accuracy: 0.5012500286102295\n",
      "LOSS train 1.3203193817536036 valid 1.3268647193908691 ACCURACY train 0.5011426210403442 validation 0.4977664649486542\n",
      "EPOCH 8:\n",
      " batch 300 training_loss: 1.3193509318431218 training_accuracy: 0.5027083158493042\n",
      " batch 600 training_loss: 1.3064260077476502 training_accuracy: 0.5087500214576721\n",
      " batch 900 training_loss: 1.321996688445409 training_accuracy: 0.49520835280418396\n",
      " batch 1200 training_loss: 1.3089629197120667 training_accuracy: 0.5104166865348816\n",
      " batch 1500 training_loss: 1.337047606309255 training_accuracy: 0.49166667461395264\n",
      " batch 1800 training_loss: 1.3147593927383423 training_accuracy: 0.5083333253860474\n",
      " batch 2100 training_loss: 1.3238760004440944 training_accuracy: 0.49812501668930054\n",
      " batch 2400 training_loss: 1.332636553645134 training_accuracy: 0.49270835518836975\n",
      " batch 2700 training_loss: 1.3239557754993438 training_accuracy: 0.5068750381469727\n",
      " batch 3000 training_loss: 1.32118210375309 training_accuracy: 0.5020833611488342\n",
      " batch 3300 training_loss: 1.3148301551739374 training_accuracy: 0.5052083730697632\n",
      " batch 3600 training_loss: 1.3387776277462642 training_accuracy: 0.4883333444595337\n",
      " batch 3900 training_loss: 1.3135840400060017 training_accuracy: 0.5056250095367432\n",
      " batch 4200 training_loss: 1.306142471432686 training_accuracy: 0.5062500238418579\n",
      " batch 4500 training_loss: 1.3167413870493572 training_accuracy: 0.5066666603088379\n",
      " batch 4800 training_loss: 1.3110413577159246 training_accuracy: 0.5058333277702332\n",
      " batch 5100 training_loss: 1.3326600587368012 training_accuracy: 0.4970833361148834\n",
      " batch 5400 training_loss: 1.3163336418072382 training_accuracy: 0.5095833539962769\n",
      " batch 5700 training_loss: 1.33960109770298 training_accuracy: 0.4866666793823242\n",
      " batch 6000 training_loss: 1.3209241630633672 training_accuracy: 0.49916666746139526\n",
      "LOSS train 1.3209241630633672 valid 1.325793981552124 ACCURACY train 0.5011439919471741 validation 0.49772849678993225\n"
     ]
    }
   ],
   "source": [
    "trainer.train_many_epochs(epochs=8, logging_frequency=300, evaluate_when_logging=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
